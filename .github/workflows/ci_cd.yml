name: CI/CD Pipeline for Data Science

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  # 1) Test job
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Check out code
        uses: actions/checkout@v3
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run tests
        run: |
          pytest tests/

      - name: Check code style
        run: |
          flake8 src/ --count --select=E9,F63,F7,F82 --show-source --statistics

  # 2) Build-and-train job
  build-and-train:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Check out code
        uses: actions/checkout@v3
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download sample dataset
        run: |
          mkdir -p data
          echo "Downloading example dataset..."
          # Use curl instead of wget for better compatibility
          curl -L https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv -o data/housing.csv
          ls -la data/

      - name: Train model and save artifacts
        run: |
          # Create models directory
          mkdir -p models
          
          echo "
          import pandas as pd
          import os
          from src.data_processing import load_data, preprocess_data, save_scaler
          from src.model import train_model, save_model
          from src.evaluate import evaluate_model

          # Load and preprocess data
          df = load_data('data/housing.csv')
          X_train, X_test, y_train, y_test, scaler = preprocess_data(df)

          # Train and save model
          model = train_model(X_train, y_train)
          save_model(model)
          save_scaler(scaler)

          # Evaluate model
          metrics = evaluate_model(model, X_test, y_test)
          print(f'Model RMSE: {metrics[\"rmse\"]}')
          print(f'Model R²: {metrics[\"r2\"]}')
          
          # Ensure the model files exist
          print('Files in models directory:')
          for file in os.listdir('models/'):
              print(f'- {file}')
          " > train.py

          # Run the training script
          python train.py
          
          # Verify models were created
          echo "Contents of models directory:"
          ls -la models/
          
          # Create a tarball of the models to avoid artifact upload issues
          tar -czvf models.tar.gz models/
          echo "Created tarball of models:"
          ls -la models.tar.gz

      # Fixed save artifacts step - no need to copy the file since it's already in the workspace
      - name: Save model artifacts
        run: |
          # Just verify the file exists and show its location
          echo "Models tarball location:"
          find $GITHUB_WORKSPACE -name "models.tar.gz"
          # Create a simple marker file to indicate success
          echo "Models successfully created" > models_ready.txt

  # 3) Build-docker job
  build-docker:
    needs: build-and-train
    runs-on: ubuntu-latest
    steps:
      - name: Check out code
        uses: actions/checkout@v3
        with:
          fetch-depth: 1

      # Alternative approach - recreate models from training script
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download sample dataset again
        run: |
          mkdir -p data
          echo "Downloading example dataset..."
          curl -L https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv -o data/housing.csv

      - name: Retrain model for Docker build
        run: |
          # Create models directory
          mkdir -p models
          
          # Run the same training script again to generate models
          # This is a workaround for the artifact download issue
          echo "
          import pandas as pd
          from src.data_processing import load_data, preprocess_data, save_scaler
          from src.model import train_model, save_model
          from src.evaluate import evaluate_model

          # Load and preprocess data
          df = load_data('data/housing.csv')
          X_train, X_test, y_train, y_test, scaler = preprocess_data(df)

          # Train and save model
          model = train_model(X_train, y_train)
          save_model(model)
          save_scaler(scaler)
          
          # Evaluate model
          metrics = evaluate_model(model, X_test, y_test)
          print(f'Model RMSE: {metrics[\"rmse\"]}')
          print(f'Model R²: {metrics[\"r2\"]}')
          " > train.py

          # Run the training script
          python train.py
          
          # Verify models were created
          echo "Contents of models directory:"
          ls -la models/

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Build Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          load: true
          tags: housing-price-predictor:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Test Docker container
        run: |
          docker run -d -p 5000:5000 --name predictor housing-price-predictor:latest
          sleep 5
          
          # Test health endpoint with retry for container startup
          for i in {1..5}; do
            response=$(curl -s http://localhost:5000/health || echo "Failed")
            echo "Health check response (attempt $i): $response"
            if [[ "$response" != "Failed" ]]; then
              break
            fi
            sleep 5
          done
          
          # Test prediction endpoint
          curl -X POST -H "Content-Type: application/json" -d '{
            "longitude": -122.23,
            "latitude": 37.88,
            "housing_median_age": 41.0,
            "total_rooms": 880.0,
            "total_bedrooms": 129.0,
            "population": 322.0,
            "households": 126.0,
            "median_income": 8.3252
          }' http://localhost:5000/predict
